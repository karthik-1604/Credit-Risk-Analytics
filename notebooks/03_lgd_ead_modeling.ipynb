{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Credit Risk Analytics - LGD, EAD and Expected Loss Modeling\n",
        "\n",
        "This notebook covers:\n",
        "- Loss Given Default (LGD) modeling using two-stage regression\n",
        "- Exposure at Default (EAD) modeling using linear regression\n",
        "- Expected Loss (EL) calculation: EL = PD x LGD x EAD\n",
        "- Portfolio-level capital requirement estimation (Basel II)\n",
        "- Stress testing under adverse economic scenarios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.metrics import roc_auc_score, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "print(\"Libraries loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_csv('../data/train_preprocessed.csv')\n",
        "test  = pd.read_csv('../data/test_preprocessed.csv')\n",
        "\n",
        "# Drop extra index column if present\n",
        "for df in [train, test]:\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "# Load saved PD model and scaler\n",
        "with open('../data/pd_model.pkl', 'rb') as f:\n",
        "    pd_model = pickle.load(f)\n",
        "\n",
        "with open('../data/scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "feature_cols = [c for c in train.columns if c != 'target']\n",
        "\n",
        "X_train = train[feature_cols].fillna(train[feature_cols].median())\n",
        "y_train = train['target']\n",
        "X_test  = test[feature_cols].fillna(test[feature_cols].median())\n",
        "y_test  = test['target']\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# Get PD predictions from saved model\n",
        "train_pd = pd_model.predict_proba(X_train_scaled)[:, 1]\n",
        "test_pd  = pd_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(f\"Training set: {train.shape}\")\n",
        "print(f\"Test set:     {test.shape}\")\n",
        "print(f\"PD loaded - Training mean PD: {train_pd.mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw = pd.read_csv(\n",
        "    '../data/accepted_2007_to_2018Q4.csv.gz',\n",
        "    compression='gzip',\n",
        "    low_memory=False,\n",
        "    nrows=500000,\n",
        "    usecols=['loan_status', 'loan_amnt', 'funded_amnt', 'total_pymnt',\n",
        "             'recoveries', 'funded_amnt_inv', 'total_rec_prncp',\n",
        "             'out_prncp', 'issue_d', 'grade', 'term', 'int_rate']\n",
        ")\n",
        "\n",
        "good_statuses = ['Fully Paid', 'Current', 'In Grace Period']\n",
        "bad_statuses  = ['Charged Off', 'Default', 'Late (31-120 days)',\n",
        "                 'Late (16-30 days)',\n",
        "                 'Does not meet the credit policy. Status:Charged Off']\n",
        "\n",
        "raw = raw[raw['loan_status'].isin(good_statuses + bad_statuses)].copy()\n",
        "raw['target'] = np.where(raw['loan_status'].isin(bad_statuses), 1, 0)\n",
        "\n",
        "# Clean numeric columns\n",
        "if raw['int_rate'].dtype == object:\n",
        "    raw['int_rate'] = raw['int_rate'].str.replace('%', '').astype(float)\n",
        "if raw['term'].dtype == object:\n",
        "    raw['term'] = raw['term'].str.extract(r'(\\d+)').astype(int)\n",
        "\n",
        "raw['issue_d'] = pd.to_datetime(raw['issue_d'], format='%b-%Y', errors='coerce')\n",
        "raw['issue_year'] = raw['issue_d'].dt.year\n",
        "\n",
        "print(f\"Raw data shape: {raw.shape}\")\n",
        "print(f\"Defaulted loans: {raw['target'].sum():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "defaulted = raw[raw['target'] == 1].copy()\n",
        "\n",
        "# Calculate LGD\n",
        "defaulted['recovery_rate'] = (defaulted['total_pymnt'] / \n",
        "                               defaulted['funded_amnt']).clip(0, 1)\n",
        "defaulted['lgd'] = 1 - defaulted['recovery_rate']\n",
        "defaulted['lgd'] = defaulted['lgd'].clip(0, 1)\n",
        "\n",
        "# Remove NaN LGD values\n",
        "defaulted = defaulted.dropna(subset=['lgd'])\n",
        "\n",
        "print(f\"Defaulted loans with LGD: {len(defaulted):,}\")\n",
        "print(f\"\\n=== LGD Distribution ===\")\n",
        "print(defaulted['lgd'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# LGD histogram\n",
        "axes[0].hist(defaulted['lgd'], bins=50, color='tomato',\n",
        "             edgecolor='black', linewidth=0.5, alpha=0.8)\n",
        "axes[0].set_xlabel('Loss Given Default (LGD)', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].set_title('LGD Distribution - Defaulted Loans', fontsize=13, fontweight='bold')\n",
        "axes[0].axvline(defaulted['lgd'].mean(), color='black', linestyle='--',\n",
        "                linewidth=2, label=f\"Mean LGD: {defaulted['lgd'].mean():.3f}\")\n",
        "axes[0].legend()\n",
        "\n",
        "# LGD by grade\n",
        "lgd_by_grade = defaulted.groupby('grade')['lgd'].mean().sort_index()\n",
        "axes[1].bar(lgd_by_grade.index, lgd_by_grade.values,\n",
        "            color='steelblue', edgecolor='black', linewidth=0.8)\n",
        "axes[1].set_xlabel('Loan Grade', fontsize=12)\n",
        "axes[1].set_ylabel('Average LGD', fontsize=12)\n",
        "axes[1].set_title('Average LGD by Loan Grade', fontsize=13, fontweight='bold')\n",
        "axes[1].set_ylim(0, 1)\n",
        "\n",
        "plt.suptitle('Loss Given Default Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('lgd_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare LGD features\n",
        "lgd_features = ['loan_amnt', 'int_rate', 'term']\n",
        "lgd_features = [f for f in lgd_features if f in defaulted.columns]\n",
        "\n",
        "# Fill missing values\n",
        "for col in lgd_features:\n",
        "    defaulted[col].fillna(defaulted[col].median(), inplace=True)\n",
        "\n",
        "X_lgd = defaulted[lgd_features].values\n",
        "y_lgd = defaulted['lgd'].values\n",
        "\n",
        "# Train/test split for LGD (temporal)\n",
        "defaulted['issue_year'] = defaulted['issue_d'].dt.year\n",
        "lgd_train_mask = defaulted['issue_year'] <= 2015\n",
        "lgd_test_mask  = defaulted['issue_year'] > 2015\n",
        "\n",
        "X_lgd_train = defaulted.loc[lgd_train_mask, lgd_features]\n",
        "X_lgd_test  = defaulted.loc[lgd_test_mask, lgd_features]\n",
        "y_lgd_train = defaulted.loc[lgd_train_mask, 'lgd']\n",
        "y_lgd_test  = defaulted.loc[lgd_test_mask, 'lgd']\n",
        "\n",
        "# Fill NaN\n",
        "X_lgd_train = X_lgd_train.fillna(X_lgd_train.median())\n",
        "X_lgd_test  = X_lgd_test.fillna(X_lgd_train.median())\n",
        "\n",
        "print(f\"LGD Training samples: {len(X_lgd_train):,}\")\n",
        "print(f\"LGD Test samples:     {len(X_lgd_test):,}\")\n",
        "\n",
        "# Stage 1: Classify total loss (LGD = 1) vs partial recovery\n",
        "y_lgd_stage1_train = (y_lgd_train == 1).astype(int)\n",
        "y_lgd_stage1_test  = (y_lgd_test == 1).astype(int)\n",
        "\n",
        "lgd_scaler = StandardScaler()\n",
        "X_lgd_train_scaled = lgd_scaler.fit_transform(X_lgd_train)\n",
        "X_lgd_test_scaled  = lgd_scaler.transform(X_lgd_test)\n",
        "\n",
        "lgd_stage1 = LogisticRegression(class_weight='balanced', max_iter=500, random_state=42)\n",
        "lgd_stage1.fit(X_lgd_train_scaled, y_lgd_stage1_train)\n",
        "\n",
        "stage1_auc = roc_auc_score(y_lgd_stage1_test, \n",
        "                            lgd_stage1.predict_proba(X_lgd_test_scaled)[:, 1])\n",
        "print(f\"\\nStage 1 (Total Loss Classification) AUC: {stage1_auc:.4f}\")\n",
        "\n",
        "# Stage 2: Regression on partial recovery cases\n",
        "partial_train = y_lgd_train < 1\n",
        "partial_test  = y_lgd_test < 1\n",
        "\n",
        "lgd_stage2 = LinearRegression()\n",
        "lgd_stage2.fit(X_lgd_train_scaled[partial_train], y_lgd_train[partial_train])\n",
        "\n",
        "stage2_r2  = r2_score(y_lgd_test[partial_test],\n",
        "                       lgd_stage2.predict(X_lgd_test_scaled[partial_test]))\n",
        "stage2_mae = mean_absolute_error(y_lgd_test[partial_test],\n",
        "                                  lgd_stage2.predict(X_lgd_test_scaled[partial_test]))\n",
        "\n",
        "print(f\"Stage 2 (Recovery Regression) R2:  {stage2_r2:.4f}\")\n",
        "print(f\"Stage 2 (Recovery Regression) MAE: {stage2_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "defaulted['ead'] = (defaulted['funded_amnt'] - \n",
        "                    defaulted['total_rec_prncp'].fillna(0)).clip(0)\n",
        "defaulted['ead_rate'] = (defaulted['ead'] / \n",
        "                          defaulted['funded_amnt']).clip(0, 1)\n",
        "\n",
        "print(\"=== EAD Distribution ===\")\n",
        "print(defaulted['ead'].describe())\n",
        "print(f\"\\nMean EAD Rate: {defaulted['ead_rate'].mean():.4f}\")\n",
        "\n",
        "# EAD model: predict EAD rate using loan features\n",
        "ead_features = ['loan_amnt', 'int_rate', 'term']\n",
        "ead_features = [f for f in ead_features if f in defaulted.columns]\n",
        "\n",
        "X_ead_train = defaulted.loc[lgd_train_mask, ead_features].fillna(\n",
        "    defaulted.loc[lgd_train_mask, ead_features].median())\n",
        "X_ead_test  = defaulted.loc[lgd_test_mask, ead_features].fillna(\n",
        "    X_ead_train.median())\n",
        "y_ead_train = defaulted.loc[lgd_train_mask, 'ead_rate']\n",
        "y_ead_test  = defaulted.loc[lgd_test_mask, 'ead_rate']\n",
        "\n",
        "ead_scaler = StandardScaler()\n",
        "X_ead_train_scaled = ead_scaler.fit_transform(X_ead_train)\n",
        "X_ead_test_scaled  = ead_scaler.transform(X_ead_test)\n",
        "\n",
        "ead_model = LinearRegression()\n",
        "ead_model.fit(X_ead_train_scaled, y_ead_train)\n",
        "\n",
        "ead_r2  = r2_score(y_ead_test, ead_model.predict(X_ead_test_scaled))\n",
        "ead_mae = mean_absolute_error(y_ead_test, ead_model.predict(X_ead_test_scaled))\n",
        "\n",
        "print(f\"\\nEAD Model R2:  {ead_r2:.4f}\")\n",
        "print(f\"EAD Model MAE: {ead_mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_lgd = defaulted['lgd'].mean()\n",
        "mean_ead_rate = defaulted['ead_rate'].mean()\n",
        "\n",
        "# Calculate EL for each loan in training set\n",
        "train_el = train_pd * mean_lgd * mean_ead_rate * train['loan_amnt'].fillna(\n",
        "    train['loan_amnt'].median())\n",
        "\n",
        "print(\"=== Expected Loss (EL) Analysis ===\")\n",
        "print(f\"Mean PD:       {train_pd.mean():.4f} ({train_pd.mean()*100:.2f}%)\")\n",
        "print(f\"Mean LGD:      {mean_lgd:.4f} ({mean_lgd*100:.2f}%)\")\n",
        "print(f\"Mean EAD Rate: {mean_ead_rate:.4f} ({mean_ead_rate*100:.2f}%)\")\n",
        "print(f\"\\nMean EL per loan: ${train_el.mean():,.2f}\")\n",
        "print(f\"Total Portfolio EL: ${train_el.sum():,.0f}\")\n",
        "print(f\"Portfolio EL Rate: {(train_el.sum() / (train['loan_amnt'].fillna(0) * len(train)/(len(train)))).mean()*100:.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518e87b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "scorecard = pd.read_csv('../data/scorecard_output.csv')\n",
        "\n",
        "el_df = pd.DataFrame({\n",
        "    'pd_score': train_pd,\n",
        "    'expected_loss': train_el.values,\n",
        "    'loan_amnt': train['loan_amnt'].fillna(train['loan_amnt'].median()).values\n",
        "})\n",
        "\n",
        "el_df['pd_decile'] = pd.qcut(el_df['pd_score'], 10, labels=False, duplicates='drop')\n",
        "\n",
        "decile_el = el_df.groupby('pd_decile').agg(\n",
        "    avg_pd=('pd_score', 'mean'),\n",
        "    avg_el=('expected_loss', 'mean'),\n",
        "    total_el=('expected_loss', 'sum')\n",
        ").round(2)\n",
        "\n",
        "print(\"=== Expected Loss by PD Decile ===\")\n",
        "print(decile_el)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Average EL by decile\n",
        "axes[0].bar(decile_el.index, decile_el['avg_el'],\n",
        "            color='tomato', edgecolor='black', linewidth=0.8)\n",
        "axes[0].set_xlabel('PD Decile (0=Lowest Risk, 9=Highest Risk)', fontsize=11)\n",
        "axes[0].set_ylabel('Average Expected Loss ($)', fontsize=11)\n",
        "axes[0].set_title('Average Expected Loss by PD Decile', fontsize=12, fontweight='bold')\n",
        "\n",
        "# EL distribution\n",
        "axes[1].hist(train_el, bins=50, color='steelblue',\n",
        "             edgecolor='black', linewidth=0.5, alpha=0.8)\n",
        "axes[1].set_xlabel('Expected Loss per Loan ($)', fontsize=11)\n",
        "axes[1].set_ylabel('Count', fontsize=11)\n",
        "axes[1].set_title('Expected Loss Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1].axvline(train_el.mean(), color='red', linestyle='--',\n",
        "                linewidth=2, label=f'Mean: ${train_el.mean():.0f}')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.suptitle('Expected Loss Analysis - EL = PD x LGD x EAD',\n",
        "             fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('expected_loss.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "535cc943",
      "metadata": {},
      "outputs": [],
      "source": [
        "scenarios = {\n",
        "    'Base Case':    {'pd_multiplier': 1.0, 'lgd_multiplier': 1.0},\n",
        "    'Mild Stress':  {'pd_multiplier': 1.5, 'lgd_multiplier': 1.2},\n",
        "    'Severe Stress':{'pd_multiplier': 2.0, 'lgd_multiplier': 1.5},\n",
        "    'Extreme Stress':{'pd_multiplier': 3.0, 'lgd_multiplier': 2.0},\n",
        "}\n",
        "\n",
        "stress_results = []\n",
        "base_el = train_el.sum()\n",
        "\n",
        "for scenario, params in scenarios.items():\n",
        "    stressed_pd  = np.clip(train_pd * params['pd_multiplier'], 0, 1)\n",
        "    stressed_lgd = min(mean_lgd * params['lgd_multiplier'], 1)\n",
        "    stressed_el  = stressed_pd * stressed_lgd * mean_ead_rate * \\\n",
        "                   train['loan_amnt'].fillna(train['loan_amnt'].median())\n",
        "    \n",
        "    stress_results.append({\n",
        "        'Scenario': scenario,\n",
        "        'PD Multiplier': params['pd_multiplier'],\n",
        "        'LGD Multiplier': params['lgd_multiplier'],\n",
        "        'Total EL ($M)': stressed_el.sum() / 1e6,\n",
        "        'EL Increase (%)': (stressed_el.sum() / base_el - 1) * 100\n",
        "    })\n",
        "\n",
        "stress_df = pd.DataFrame(stress_results)\n",
        "print(\"=== Stress Test Results ===\")\n",
        "print(stress_df.to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "colors = ['steelblue', 'orange', 'tomato', 'darkred']\n",
        "bars = ax.bar(stress_df['Scenario'], stress_df['Total EL ($M)'],\n",
        "              color=colors, edgecolor='black', linewidth=0.8)\n",
        "ax.set_ylabel('Total Expected Loss ($M)', fontsize=12)\n",
        "ax.set_title('Stress Test - Expected Loss Under Different Scenarios',\n",
        "             fontsize=13, fontweight='bold')\n",
        "for bar, val in zip(bars, stress_df['Total EL ($M)']):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "            f'${val:.1f}M', ha='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.savefig('stress_test.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2037aae",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('../data/lgd_stage1_model.pkl', 'wb') as f:\n",
        "    pickle.dump(lgd_stage1, f)\n",
        "\n",
        "with open('../data/lgd_stage2_model.pkl', 'wb') as f:\n",
        "    pickle.dump(lgd_stage2, f)\n",
        "\n",
        "with open('../data/ead_model.pkl', 'wb') as f:\n",
        "    pickle.dump(ead_model, f)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\"  ../data/lgd_stage1_model.pkl\")\n",
        "print(\"  ../data/lgd_stage2_model.pkl\")\n",
        "print(\"  ../data/ead_model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LGD, EAD and Expected Loss Summary\n",
        "\n",
        "Complete Basel-style EL pipeline implemented.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
